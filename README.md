# ğŸ§  Neural-Sync Language Lab

> **Activate Your Voice Hackathon â€” Feb 28 â€“ Mar 1, 2026**
> Track 1: Communication & Human Experience

---

## ğŸ¯ Vision

Neural-Sync Language Lab reimagines language learning by shifting focus from isolated vocabulary to a **dynamic, sentence-based neural ecosystem**.

Utilizing a real-time AI voice interface, the platform maps your **"linguistic borders"** â€” the edge of what you can express â€” and provides **i+1 adaptive input** (Krashen's theory), forcing the active retrieval of past sentences to prevent decay.

Through a stunning, interactive **Knowledge Graph**, users visually track their growing neural network as simple greetings evolve into complex fluency, ensuring that every learned structure is **permanently hardwired through contextual activation**.

---

## ğŸ’¡ How It Aligns With the Hackathon Mission

| Hackathon Criteria | Neural-Sync Implementation |
|---|---|
| **Intelligent Actions** | i+1 adaptive input that pushes the learner just beyond their current level â€” not passive flashcards, but proactive sentence generation |
| **Deep Memory** | Persistent memory of every sentence learned, mastery level, and decay risk via Backboard.io's stateful memory layer |
| **Adaptive Behavior Intelligence** | Real-time mapping of "linguistic borders" that evolves per user â€” the system personalizes its approach based on strengths, weaknesses, and learning velocity |
| **Continuous Improvement** | Spaced retrieval forcing reactivation of past structures; the Knowledge Graph densifies with every conversation, and the system gets smarter over time |

---

## ğŸ—ï¸ Architecture Overview

```
â”‚                    FRONTEND                          â”‚
â”‚  React App + D3.js Knowledge Graph + Voice UI        â”‚
â”‚  - Mic capture & audio streaming                     â”‚
â”‚  - Interactive neural graph visualization            â”‚
â”‚  - Session dashboard & progress metrics              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                  â”‚
               â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SPEECHMATICS API   â”‚  â”‚      VOICE SYNTHESIS       â”‚
â”‚  Real-time STT       â”‚  â”‚  (Web Speech API / OpenAI  â”‚
â”‚  - Multilingual      â”‚  â”‚   TTS for responses)       â”‚
â”‚  - Pronunciation     â”‚  â”‚                            â”‚
â”‚    confidence scores  â”‚  â”‚                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ORCHESTRATION LAYER                   â”‚
â”‚  Python/Node backend                                 â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  BACKBOARD.IO    â”‚    â”‚  OPENAI GPT-5.3          â”‚ â”‚
â”‚  â”‚  Memory Layer    â”‚    â”‚  Intelligence Engine     â”‚ â”‚
â”‚  â”‚                  â”‚    â”‚                          â”‚ â”‚
â”‚  â”‚ - User profile   â”‚â—„â”€â”€â–ºâ”‚ - i+1 sentence gen      â”‚ â”‚
â”‚  â”‚ - Learned items  â”‚    â”‚ - Level assessment       â”‚ â”‚
â”‚  â”‚ - Mastery scores â”‚    â”‚ - Border mapping         â”‚ â”‚
â”‚  â”‚ - Decay tracking â”‚    â”‚ - Retrieval scheduling   â”‚ â”‚
â”‚  â”‚ - Entity graph   â”‚    â”‚ - Conversation flow      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Tech Stack

### Core Sponsors (all three integrated)

| Tool | Role | Why |
|---|---|---|
| **Speechmatics** | Real-time Speech-to-Text | Multilingual recognition, pronunciation confidence scoring, language detection. The "ear" of the system. |
| **Backboard.io** | Persistent AI Memory | Stores learner profiles, sentence mastery, decay timers, entity relationships across sessions. The "long-term brain." SDK: `pip install backboard-sdk` / `npm install backboard-sdk` |
| **OpenAI GPT-5.3** | Adaptive Intelligence | Generates i+1 sentences, evaluates learner level, manages pedagogical logic, orchestrates retrieval. The "reasoning engine." |

### Frontend

| Tool | Role |
|---|---|
| **React** (or Next.js) | Main application shell |
| **D3.js** / **vis.js** | Interactive Knowledge Graph visualization |
| **Web Audio API** | Mic capture + audio streaming to Speechmatics |
| **Web Speech API** / **OpenAI TTS** | Voice output for the tutor |

### Backend

| Tool | Role |
|---|---|
| **Python (FastAPI)** or **Node.js (Express)** | Orchestration server |
| **WebSocket** | Real-time bidirectional audio/text streaming |

---

## ğŸ§ª Core Features (MVP Scope for 24h)

### 1. Voice Conversation Loop (Priority 1 â€” MUST HAVE)
- User speaks in target language via microphone
- Speechmatics transcribes in real-time
- GPT evaluates the sentence, identifies level, generates the next i+1 prompt
- Backboard stores the interaction with mastery metadata
- TTS speaks the response back to the user

### 2. Linguistic Border Mapping (Priority 1 â€” MUST HAVE)
- After a few exchanges, the system identifies what the user CAN and CANNOT express
- Builds an internal "border map" of linguistic competence
- Uses this map to always push i+1 â€” not too easy, not too hard

### 3. Active Retrieval & Decay Prevention (Priority 2 â€” SHOULD HAVE)
- Backboard tracks when each sentence was last activated
- System periodically forces retrieval of "at-risk" structures in new contexts
- Example: user learned "Je voudrais un cafÃ©" 30 min ago â†’ system generates a new situation requiring "Je voudrais..." in a different context

### 4. Knowledge Graph Visualization (Priority 2 â€” SHOULD HAVE)
- Interactive D3.js graph showing:
  - Nodes = learned sentences/structures
  - Edges = shared vocabulary/grammar links
  - Color = mastery level (green = solid, yellow = at risk, red = decaying)
  - Size = usage frequency
- Real-time updates as the user learns

### 5. Session Dashboard (Priority 3 â€” NICE TO HAVE)
- Stats: sentences learned, mastery %, session duration
- Progress over time
- Linguistic border expansion visualization

---

## ğŸ—ºï¸ 24-Hour Roadmap

### Phase 1: Foundation 
- [ ] Project scaffolding (frontend + backend)
- [ ] Speechmatics WebSocket integration â€” mic â†’ STT working
- [ ] Backboard.io setup â€” assistant + thread creation
- [ ] OpenAI API connection â€” basic prompt/response
- [ ] **Checkpoint: can speak into mic and get a text transcription + AI response**

### Phase 2: Core Loop
- [ ] Full voice conversation loop: speak â†’ transcribe â†’ GPT process â†’ TTS respond
- [ ] Backboard memory integration: store each sentence with metadata (mastery, timestamp, grammar tags)
- [ ] i+1 logic: GPT system prompt that uses Backboard memory to generate the next appropriate sentence
- [ ] Linguistic border detection: after 5+ exchanges, system builds a competence profile
- [ ] **Checkpoint: full voice learning session works end-to-end**

### Phase 3: Intelligence & Memory 
- [ ] Decay tracking: tag sentences with "last activated" timestamps in Backboard
- [ ] Retrieval forcing: system weaves old structures into new prompts
- [ ] Refine i+1 adaptation based on accumulated border data
- [ ] Error correction flow: detect pronunciation/grammar issues and provide feedback
- [ ] **Checkpoint: system demonstrates memory and adaptation across a multi-turn session**

### Phase 4: Knowledge Graph & Polish
- [ ] D3.js Knowledge Graph: nodes, edges, color coding
- [ ] Real-time graph updates during conversation
- [ ] Session dashboard with key metrics
- [ ] UI/UX polish â€” make it demo-ready
- [ ] **Checkpoint: visually stunning, demo-ready product**

### Phase 5: Demo Prep
- [ ] Prepare demo script (2-3 min live demo)
- [ ] Edge case testing
- [ ] Pitch deck / slides if needed
- [ ] Record backup demo video

---

## ğŸ“‚ Project Structure (Proposed)

```
neural-sync/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ VoiceInterface.jsx      # Mic + audio controls
â”‚   â”‚   â”‚   â”œâ”€â”€ KnowledgeGraph.jsx      # D3.js graph
â”‚   â”‚   â”‚   â”œâ”€â”€ ConversationPanel.jsx   # Live transcript
â”‚   â”‚   â”‚   â””â”€â”€ Dashboard.jsx           # Stats & progress
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ useSpeechmatics.js      # Speechmatics WebSocket
â”‚   â”‚   â”‚   â””â”€â”€ useAudioCapture.js      # Mic stream
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js                  # Backend API calls
â”‚   â”‚   â””â”€â”€ App.jsx
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                         # FastAPI server
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ speechmatics_service.py     # STT integration
â”‚   â”‚   â”œâ”€â”€ backboard_service.py        # Memory layer
â”‚   â”‚   â”œâ”€â”€ openai_service.py           # GPT intelligence
â”‚   â”‚   â””â”€â”€ orchestrator.py             # Core learning loop
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py                  # Data models
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ .env                                # API keys (DO NOT COMMIT)
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## ğŸ”‘ Environment Variables

```env
# Speechmatics
SPEECHMATICS_API_KEY=your_key_here

# Backboard.io
BACKBOARD_API_KEY=bk_your_key_here

# OpenAI
OPENAI_API_KEY=sk-your_key_here
OPENAI_ORG_ID=org-your_org_here
```

---

## ğŸ† Why We Win

1. **All 3 sponsors deeply integrated** â€” not token usage, real architectural dependency
2. **Voice-first by design** â€” not a text app with a mic bolted on
3. **Memory is the product** â€” Backboard isn't a nice-to-have, it IS the learning engine
4. **Visually striking** â€” the Knowledge Graph is an instant "wow" for judges
5. **Grounded in real science** â€” Krashen's i+1, spaced retrieval, active recall
6. **Real-world utility** â€” language learning is a massive market, this is a credible product

---
